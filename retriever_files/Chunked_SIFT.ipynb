{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f7732e36a36441fbb55292b750f07814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d57392b1b66c4e4ba528a7667e3f5898",
              "IPY_MODEL_768dca8afc4d49968b07ce4202d591d7",
              "IPY_MODEL_6ff5d40da0794e36a495d99cdd3657aa"
            ],
            "layout": "IPY_MODEL_392c62853cb642d4b7f7593bce74dffd"
          }
        },
        "d57392b1b66c4e4ba528a7667e3f5898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7211f2365a424fdc9d22ae97535a27fe",
            "placeholder": "​",
            "style": "IPY_MODEL_a1362c3bd3634c0a8cf324084d5df79a",
            "value": "Downloading readme: 100%"
          }
        },
        "768dca8afc4d49968b07ce4202d591d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_502d565d15a9417db56eed7994a5f9d6",
            "max": 653,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f8327fa4f304a1cbb3dded9d66d9c2f",
            "value": 653
          }
        },
        "6ff5d40da0794e36a495d99cdd3657aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831195afe0a24c5fa80d8fc535d4da52",
            "placeholder": "​",
            "style": "IPY_MODEL_c80dce8edc324dc69c981c2b975a2724",
            "value": " 653/653 [00:00&lt;00:00, 44.3kB/s]"
          }
        },
        "392c62853cb642d4b7f7593bce74dffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7211f2365a424fdc9d22ae97535a27fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1362c3bd3634c0a8cf324084d5df79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "502d565d15a9417db56eed7994a5f9d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8327fa4f304a1cbb3dded9d66d9c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "831195afe0a24c5fa80d8fc535d4da52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80dce8edc324dc69c981c2b975a2724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8e7d684440042b88cd036ccee8a9b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ff22763d2834223929a98719e998888",
              "IPY_MODEL_cc773b505dc14dcbaaf0a5fe96520dea",
              "IPY_MODEL_544e29e564ab4767b9debb3798ae367a"
            ],
            "layout": "IPY_MODEL_0e8c366ccf624710bb110d4b6d48fafc"
          }
        },
        "1ff22763d2834223929a98719e998888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f9e4994711486a934bf2317f15cbb7",
            "placeholder": "​",
            "style": "IPY_MODEL_ae1f088accde4eb4a243d98b5f2120b0",
            "value": "Downloading data: 100%"
          }
        },
        "cc773b505dc14dcbaaf0a5fe96520dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4adfc1664b14a29a7928474dd79f95d",
            "max": 24690,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f5ca34d6c7540a3a63872c7ebc85267",
            "value": 24690
          }
        },
        "544e29e564ab4767b9debb3798ae367a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dab0e328167342d89946a4664d98985c",
            "placeholder": "​",
            "style": "IPY_MODEL_862cf28b84b64d7e8b11ab19f6eb11e1",
            "value": " 24.7k/24.7k [00:00&lt;00:00, 81.5kB/s]"
          }
        },
        "0e8c366ccf624710bb110d4b6d48fafc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f9e4994711486a934bf2317f15cbb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae1f088accde4eb4a243d98b5f2120b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4adfc1664b14a29a7928474dd79f95d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f5ca34d6c7540a3a63872c7ebc85267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dab0e328167342d89946a4664d98985c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "862cf28b84b64d7e8b11ab19f6eb11e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "047e552a8253469d9e5e097c49d5edb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e296b293ce68432ebf31eb77d61a5732",
              "IPY_MODEL_e500e817ab70456fba2d085596e02eaa",
              "IPY_MODEL_1a2a4c18f38842208fcbd3afa1587212"
            ],
            "layout": "IPY_MODEL_99feaccd0f7249d4a21bbff6d3fac551"
          }
        },
        "e296b293ce68432ebf31eb77d61a5732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0499de49154b43b0b263773bd6207d",
            "placeholder": "​",
            "style": "IPY_MODEL_5f3ad731478f41b6bd62b08dabe28e9d",
            "value": "Generating train split: 100%"
          }
        },
        "e500e817ab70456fba2d085596e02eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fab9a7baecf442e2a621a8e9dfd9e3b0",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b283252ec8e14c5d919dd3c3dd270062",
            "value": 24
          }
        },
        "1a2a4c18f38842208fcbd3afa1587212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7af7f5fd7b284814be0301ced377e41a",
            "placeholder": "​",
            "style": "IPY_MODEL_507bc499d5fa453ca09187d77a278fc2",
            "value": " 24/24 [00:00&lt;00:00, 370.36 examples/s]"
          }
        },
        "99feaccd0f7249d4a21bbff6d3fac551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b0499de49154b43b0b263773bd6207d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f3ad731478f41b6bd62b08dabe28e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fab9a7baecf442e2a621a8e9dfd9e3b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b283252ec8e14c5d919dd3c3dd270062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7af7f5fd7b284814be0301ced377e41a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "507bc499d5fa453ca09187d77a278fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Downloads"
      ],
      "metadata": {
        "id": "iRjv2Lr-R_ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers datasets sentence-transformers evaluate tqdm bitsandbytes faiss-cpu activeft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gFMclY7-W3E",
        "outputId": "720d780d-1f84-4c6e-ca98-f098a8db64b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index-postprocessor-flag-embedding-reranker\n",
        "!pip install -q git+https://github.com/FlagOpen/FlagEmbedding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn4io1pU-XcB",
        "outputId": "5c9b24ce-e239-416a-c3ad-232f967c7045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.4/1.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.9/347.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index==0.9.40"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Pmn-u--bMY",
        "outputId": "fbcd3a55-12ef-4d6b-f452-d559cf4a349f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell twice for it to work\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict\n",
        "from datasets import load_dataset\n",
        "from llama_index.text_splitter import SentenceSplitter\n",
        "from llama_index.schema import Document\n",
        "from activeft.sift import Retriever\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "q_oisSmTKQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "l47uDlbISC69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_method(cls):\n",
        "    def decorator(func):\n",
        "        setattr(cls, func.__name__, func)\n",
        "        return func\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "hpP3l2_oLDAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reranker:\n",
        "    def __init__(self, model_name=\"BAAI/bge-reranker-large\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "    def rerank(self, query: str, passages: List[Dict], top_n: int = 5) -> List[Dict]:\n",
        "        pairs = [(query, doc['text']) for doc in passages]\n",
        "        features = self.tokenizer(\n",
        "            pairs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            scores = self.model(**features).logits.squeeze().cpu().numpy()\n",
        "\n",
        "        scored_docs = [(float(score), doc) for score, doc in zip(scores, passages)]\n",
        "        scored_docs.sort(reverse=True)\n",
        "\n",
        "        reranked_docs = []\n",
        "        for score, doc in scored_docs[:top_n]:\n",
        "            doc_copy = doc.copy()\n",
        "            doc_copy['score'] = score\n",
        "            reranked_docs.append(doc_copy)\n",
        "\n",
        "        return reranked_docs"
      ],
      "metadata": {
        "id": "ziYgQ1KVKptT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SIFTRetriever:\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model_name='Snowflake/snowflake-arctic-embed-l',\n",
        "        k=5,\n",
        "        chunk_size=256,\n",
        "        lambda_param=0.01,\n",
        "        also_query_opposite=True,\n",
        "        rerank=True,\n",
        "        top_k_rerank=5,\n",
        "        threads=1,\n",
        "        noise_std=None\n",
        "    ):\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.text_splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=25)\n",
        "        self.k = k\n",
        "        self.lambda_param = lambda_param\n",
        "        self.also_query_opposite = also_query_opposite\n",
        "        self.threads = threads\n",
        "        self.chunks = []\n",
        "        self.retriever = None\n",
        "        self.rerank = rerank\n",
        "        self.top_k_rerank = top_k_rerank\n",
        "        self.noise_std = noise_std if noise_std is not None else np.sqrt(lambda_param)\n",
        "\n",
        "        if rerank:\n",
        "            self.reranker = Reranker()"
      ],
      "metadata": {
        "id": "Ykshu4f4MR9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(SIFTRetriever)\n",
        "def load_corpus(self, corpus_path: str):\n",
        "    \"\"\"Load and prepare corpus for indexing\"\"\"\n",
        "    print(\"Loading corpus...\")\n",
        "    with open(corpus_path, 'r') as f:\n",
        "        corpus_data = json.load(f)\n",
        "\n",
        "    print(\"Processing documents and creating chunks...\")\n",
        "    for doc in tqdm(corpus_data):\n",
        "        content = doc.get('body', '')\n",
        "        if not content:\n",
        "            continue\n",
        "\n",
        "        document = Document(\n",
        "            text=content,\n",
        "            metadata={\n",
        "                'title': doc.get('title', ''),\n",
        "                'source': doc.get('source', ''),\n",
        "                'published_at': doc.get('published_at', ''),\n",
        "                'category': doc.get('category', '')\n",
        "            }\n",
        "        )\n",
        "\n",
        "        doc_chunks = self.text_splitter.split_text(document.text)\n",
        "\n",
        "        for chunk in doc_chunks:\n",
        "            self.chunks.append({\n",
        "                'text': chunk,\n",
        "                'metadata': document.metadata\n",
        "            })\n",
        "\n",
        "    self._build_index()\n"
      ],
      "metadata": {
        "id": "fDkZhYcpMUZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run for VTL\n"
      ],
      "metadata": {
        "id": "8Bm95budR4lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(SIFTRetriever)\n",
        "def _build_index(self):\n",
        "    \"\"\"Build FAISS index with SIFT retriever\"\"\"\n",
        "    print(\"Building FAISS index...\")\n",
        "    embeddings = []\n",
        "    valid_chunks = []\n",
        "\n",
        "    for chunk in tqdm(self.chunks):\n",
        "        try:\n",
        "            embedding = self.embedding_model.encode(chunk['text'], convert_to_tensor=True)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "            valid_chunks.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding chunk: {e}\")\n",
        "            continue\n",
        "\n",
        "    self.chunks = valid_chunks\n",
        "    embeddings = np.array(embeddings).astype('float32')\n",
        "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatIP(dimension)  # Using IP (Inner Product) as per SIFT\n",
        "    faiss_index.add(embeddings)\n",
        "    print(f\"Created FAISS index with {len(embeddings)} embeddings\")\n",
        "\n",
        "    self.retriever = Retriever(\n",
        "        index=faiss_index,\n",
        "        llambda=self.lambda_param,\n",
        "        fast=False,\n",
        "        also_query_opposite=self.also_query_opposite,\n",
        "        only_faiss=False\n",
        "    )"
      ],
      "metadata": {
        "id": "soGwWtUbOVvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(SIFTRetriever)\n",
        "def retrieve(self, query: str) -> List[Dict]:\n",
        "    \"\"\"Retrieve chunks using SIFT with optional reranking\"\"\"\n",
        "    query_embedding = self.embedding_model.encode(query).astype('float32')\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    D, I, V, retrieval_time = self.retriever.search(\n",
        "        query=query_embedding,\n",
        "        N=self.k if not self.rerank else self.k * 2,  # Retrieve more if reranking\n",
        "        K=None,\n",
        "        mean_pooling=False,\n",
        "        threads=self.threads\n",
        "    )\n",
        "\n",
        "    # Handle both single result and array results\n",
        "    if isinstance(I, np.int64):\n",
        "        indices = [I]\n",
        "        scores = [D]\n",
        "    else:\n",
        "        indices = I[0] if I.ndim > 1 else I\n",
        "        scores = D[0] if D.ndim > 1 else D\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(indices, scores):\n",
        "        chunk = self.chunks[idx]\n",
        "        results.append({\n",
        "            'text': chunk['text'],\n",
        "            'score': float(score),\n",
        "            'metadata': chunk['metadata']\n",
        "        })\n",
        "\n",
        "    if self.rerank:\n",
        "        results = self.reranker.rerank(query, results, self.top_k_rerank)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "IBKfkhCLOp2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    rerank = True\n",
        "    top_k = 5\n",
        "    chunk_size = 256\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = SIFTRetriever(\n",
        "        k=20 if rerank else top_k,\n",
        "        chunk_size=chunk_size,\n",
        "        rerank=rerank,\n",
        "        top_k_rerank=top_k\n",
        "    )\n",
        "\n",
        "    # Load corpus\n",
        "    corpus_path = '/content/corpus.json'\n",
        "    retriever.load_corpus(corpus_path)\n",
        "\n",
        "    # Load dataset from Hugging Face\n",
        "    print(\"Loading dataset from Hugging Face...\")\n",
        "    dataset = load_dataset(\"yobro4619/multihop_rag_balanced_sample\")\n",
        "    queries = dataset['train']\n",
        "\n",
        "    results = []\n",
        "    print(\"Performing retrieval...\")\n",
        "    for query_item in tqdm(queries):\n",
        "        retrieved_docs = retriever.retrieve(query_item['query'])\n",
        "\n",
        "        result = {\n",
        "            'query': query_item['query'],\n",
        "            'answer': query_item['answer'],\n",
        "            'question_type': query_item['question_type'],\n",
        "            'retrieval_list': retrieved_docs,\n",
        "            'gold_list': query_item.get('evidence_list', [])\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    output_file = '/content/enhanced_sift_retrieval.json'\n",
        "    print(f\"Saving results to {output_file}\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "f7732e36a36441fbb55292b750f07814",
            "d57392b1b66c4e4ba528a7667e3f5898",
            "768dca8afc4d49968b07ce4202d591d7",
            "6ff5d40da0794e36a495d99cdd3657aa",
            "392c62853cb642d4b7f7593bce74dffd",
            "7211f2365a424fdc9d22ae97535a27fe",
            "a1362c3bd3634c0a8cf324084d5df79a",
            "502d565d15a9417db56eed7994a5f9d6",
            "3f8327fa4f304a1cbb3dded9d66d9c2f",
            "831195afe0a24c5fa80d8fc535d4da52",
            "c80dce8edc324dc69c981c2b975a2724",
            "b8e7d684440042b88cd036ccee8a9b6a",
            "1ff22763d2834223929a98719e998888",
            "cc773b505dc14dcbaaf0a5fe96520dea",
            "544e29e564ab4767b9debb3798ae367a",
            "0e8c366ccf624710bb110d4b6d48fafc",
            "53f9e4994711486a934bf2317f15cbb7",
            "ae1f088accde4eb4a243d98b5f2120b0",
            "d4adfc1664b14a29a7928474dd79f95d",
            "5f5ca34d6c7540a3a63872c7ebc85267",
            "dab0e328167342d89946a4664d98985c",
            "862cf28b84b64d7e8b11ab19f6eb11e1",
            "047e552a8253469d9e5e097c49d5edb3",
            "e296b293ce68432ebf31eb77d61a5732",
            "e500e817ab70456fba2d085596e02eaa",
            "1a2a4c18f38842208fcbd3afa1587212",
            "99feaccd0f7249d4a21bbff6d3fac551",
            "0b0499de49154b43b0b263773bd6207d",
            "5f3ad731478f41b6bd62b08dabe28e9d",
            "fab9a7baecf442e2a621a8e9dfd9e3b0",
            "b283252ec8e14c5d919dd3c3dd270062",
            "7af7f5fd7b284814be0301ced377e41a",
            "507bc499d5fa453ca09187d77a278fc2"
          ]
        },
        "id": "YMbOrA-O-fCe",
        "outputId": "89fd442c-9a42-45b6-9a5f-3a493d4abe3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpus...\n",
            "Processing documents and creating chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 609/609 [00:03<00:00, 162.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6495/6495 [06:09<00:00, 17.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings shape: (6495, 1024)\n",
            "Created FAISS index with 6495 embeddings\n",
            "Loading dataset from Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/653 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7732e36a36441fbb55292b750f07814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/24.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8e7d684440042b88cd036ccee8a9b6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/24 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "047e552a8253469d9e5e097c49d5edb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing retrieval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [01:32<00:00,  3.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/enhanced_sift_retrieval.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "GgSR6lkBvnjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run for CosineSimilarity"
      ],
      "metadata": {
        "id": "HmTqwhnQSNPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from activeft.acquisition_functions.cosine_similarity import CosineSimilarity\n",
        "\n",
        "@add_method(SIFTRetriever)\n",
        "def _build_index(self):\n",
        "    \"\"\"Build FAISS index with SIFT retriever\"\"\"\n",
        "    print(\"Building FAISS index...\")\n",
        "    embeddings = []\n",
        "    valid_chunks = []\n",
        "\n",
        "    for chunk in tqdm(self.chunks):\n",
        "        try:\n",
        "            embedding = self.embedding_model.encode(chunk['text'], convert_to_tensor=True)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "            valid_chunks.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding chunk: {e}\")\n",
        "            continue\n",
        "\n",
        "    self.chunks = valid_chunks\n",
        "    embeddings = np.array(embeddings).astype('float32')\n",
        "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatIP(dimension)  # Using IP (Inner Product) as per SIFT\n",
        "    faiss_index.add(embeddings)\n",
        "    print(f\"Created FAISS index with {len(embeddings)} embeddings\")\n",
        "\n",
        "    cosine_similarity = CosineSimilarity(\n",
        "            target=torch.Tensor(),\n",
        "            mini_batch_size=1000,\n",
        "            embedding_batch_size=32\n",
        "        )\n",
        "\n",
        "    self.retriever = Retriever(\n",
        "        index=faiss_index,\n",
        "        llambda=self.lambda_param,\n",
        "        fast=False,\n",
        "        also_query_opposite=self.also_query_opposite,\n",
        "        only_faiss=False,\n",
        "        acquisition_function=cosine_similarity\n",
        "    )"
      ],
      "metadata": {
        "id": "EtNQhkRSSPmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(SIFTRetriever)\n",
        "def retrieve(self, query: str) -> List[Dict]:\n",
        "    \"\"\"Retrieve chunks using SIFT with optional reranking\"\"\"\n",
        "    query_embedding = self.embedding_model.encode(query).astype('float32')\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    D, I, V, retrieval_time = self.retriever.search(\n",
        "        query=query_embedding,\n",
        "        N=self.k if not self.rerank else self.k * 2,  # Retrieve more if reranking\n",
        "        K=None,\n",
        "        mean_pooling=False,\n",
        "        threads=self.threads\n",
        "    )\n",
        "\n",
        "    # Handle both single result and array results\n",
        "    if isinstance(I, np.int64):\n",
        "        indices = [I]\n",
        "        scores = [D]\n",
        "    else:\n",
        "        indices = I[0] if I.ndim > 1 else I\n",
        "        scores = D[0] if D.ndim > 1 else D\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(indices, scores):\n",
        "        chunk = self.chunks[idx]\n",
        "        results.append({\n",
        "            'text': chunk['text'],\n",
        "            'score': float(score),\n",
        "            'metadata': chunk['metadata']\n",
        "        })\n",
        "\n",
        "    if self.rerank:\n",
        "        results = self.reranker.rerank(query, results, self.top_k_rerank)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Cgw3QtX8SUBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    rerank = True\n",
        "    top_k = 5\n",
        "    chunk_size = 256\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = SIFTRetriever(\n",
        "        k=20 if rerank else top_k,\n",
        "        chunk_size=chunk_size,\n",
        "        rerank=rerank,\n",
        "        top_k_rerank=top_k\n",
        "    )\n",
        "\n",
        "    # Load corpus\n",
        "    corpus_path = '/content/corpus.json'\n",
        "    retriever.load_corpus(corpus_path)\n",
        "\n",
        "    # Load dataset from Hugging Face\n",
        "    print(\"Loading dataset from Hugging Face...\")\n",
        "    dataset = load_dataset(\"yobro4619/multihop_rag_balanced_sample\")\n",
        "    queries = dataset['train']\n",
        "\n",
        "    results = []\n",
        "    print(\"Performing retrieval...\")\n",
        "    for query_item in tqdm(queries):\n",
        "        retrieved_docs = retriever.retrieve(query_item['query'])\n",
        "\n",
        "        result = {\n",
        "            'query': query_item['query'],\n",
        "            'answer': query_item['answer'],\n",
        "            'question_type': query_item['question_type'],\n",
        "            'retrieval_list': retrieved_docs,\n",
        "            'gold_list': query_item.get('evidence_list', [])\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    output_file = '/content/sift_.CosineSimilarity.json'\n",
        "    print(f\"Saving results to {output_file}\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "bUaJD8m7SW9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwWrfd6JSXas",
        "outputId": "316cff6c-7d01-4d17-f9a2-5aa854ec1165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpus...\n",
            "Processing documents and creating chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 609/609 [00:03<00:00, 159.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6495/6495 [05:52<00:00, 18.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings shape: (6495, 1024)\n",
            "Created FAISS index with 6495 embeddings\n",
            "Loading dataset from Hugging Face...\n",
            "Performing retrieval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [01:09<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/enhanced_sift_retrieval.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run for ITL"
      ],
      "metadata": {
        "id": "V4z0fVFQamO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from activeft.acquisition_functions.itl import ITL\n",
        "from activeft.acquisition_functions.bace import BaCEState\n",
        "\n",
        "@add_method(SIFTRetriever)\n",
        "def _build_index(self):\n",
        "    \"\"\"Build FAISS index with SIFT retriever\"\"\"\n",
        "    print(\"Building FAISS index...\")\n",
        "    embeddings = []\n",
        "    valid_chunks = []\n",
        "\n",
        "    for chunk in tqdm(self.chunks):\n",
        "        try:\n",
        "            embedding = self.embedding_model.encode(chunk['text'], convert_to_tensor=True)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "            valid_chunks.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding chunk: {e}\")\n",
        "            continue\n",
        "\n",
        "    self.chunks = valid_chunks\n",
        "    embeddings = np.array(embeddings).astype('float32')\n",
        "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatIP(dimension)  # Using IP (Inner Product) as per SIFT\n",
        "    faiss_index.add(embeddings)\n",
        "    print(f\"Created FAISS index with {len(embeddings)} embeddings\")\n",
        "\n",
        "    itl = ITL(\n",
        "            target=torch.Tensor(),  # Will be set during retrieval\n",
        "            noise_std=self.noise_std,\n",
        "            mini_batch_size=1000,\n",
        "        )\n",
        "\n",
        "    self.retriever = Retriever(\n",
        "            index=faiss_index,\n",
        "            llambda=self.lambda_param,\n",
        "            fast=False,\n",
        "            also_query_opposite=self.also_query_opposite,\n",
        "            only_faiss=False,\n",
        "            acquisition_function=itl\n",
        "        )"
      ],
      "metadata": {
        "id": "P0W_Ufu8altI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(SIFTRetriever)\n",
        "def retrieve(self, query: str) -> List[Dict]:\n",
        "    \"\"\"Retrieve chunks using SIFT with optional reranking\"\"\"\n",
        "    query_embedding = self.embedding_model.encode(query).astype('float32')\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    D, I, V, retrieval_time = self.retriever.search(\n",
        "        query=query_embedding,\n",
        "        N=self.k if not self.rerank else self.k * 2,  # Retrieve more if reranking\n",
        "        K=None,\n",
        "        mean_pooling=False,\n",
        "        threads=self.threads\n",
        "    )\n",
        "\n",
        "    # Handle both single result and array results\n",
        "    if isinstance(I, np.int64):\n",
        "        indices = [I]\n",
        "        scores = [D]\n",
        "    else:\n",
        "        indices = I[0] if I.ndim > 1 else I\n",
        "        scores = D[0] if D.ndim > 1 else D\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(indices, scores):\n",
        "        chunk = self.chunks[idx]\n",
        "        results.append({\n",
        "            'text': chunk['text'],\n",
        "            'score': float(score),\n",
        "            'metadata': chunk['metadata']\n",
        "        })\n",
        "\n",
        "    if self.rerank:\n",
        "        results = self.reranker.rerank(query, results, self.top_k_rerank)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "epPLcpEmbibH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    rerank = True\n",
        "    top_k = 5\n",
        "    chunk_size = 256\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = SIFTRetriever(\n",
        "        k=20 if rerank else top_k,\n",
        "        chunk_size=chunk_size,\n",
        "        rerank=rerank,\n",
        "        top_k_rerank=top_k\n",
        "    )\n",
        "\n",
        "    # Load corpus\n",
        "    corpus_path = '/content/corpus.json'\n",
        "    retriever.load_corpus(corpus_path)\n",
        "\n",
        "    # Load dataset from Hugging Face\n",
        "    print(\"Loading dataset from Hugging Face...\")\n",
        "    dataset = load_dataset(\"yobro4619/multihop_rag_balanced_sample\")\n",
        "    queries = dataset['train']\n",
        "\n",
        "    results = []\n",
        "    print(\"Performing retrieval...\")\n",
        "    for query_item in tqdm(queries):\n",
        "        retrieved_docs = retriever.retrieve(query_item['query'])\n",
        "\n",
        "        result = {\n",
        "            'query': query_item['query'],\n",
        "            'answer': query_item['answer'],\n",
        "            'question_type': query_item['question_type'],\n",
        "            'retrieval_list': retrieved_docs,\n",
        "            'gold_list': query_item.get('evidence_list', [])\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    output_file = '/content/sift_.itl.json'\n",
        "    print(f\"Saving results to {output_file}\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "d2RqhbfdbqB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9_JyUP6bs4Q",
        "outputId": "11c1201d-9e31-41e7-e0d0-ca49d17da14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpus...\n",
            "Processing documents and creating chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 609/609 [00:03<00:00, 165.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6495/6495 [05:54<00:00, 18.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings shape: (6495, 1024)\n",
            "Created FAISS index with 6495 embeddings\n",
            "Loading dataset from Hugging Face...\n",
            "Performing retrieval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [01:27<00:00,  3.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/sift_.itl.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run for LazyVTL"
      ],
      "metadata": {
        "id": "U_i1CtU0f46p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "from activeft.model import ModelWithEmbeddingOrKernel\n",
        "class PatchedLazyVTL(LazyVTL):\n",
        "  def select_from_minibatch(\n",
        "          self,\n",
        "          batch_size: int,\n",
        "          model: ModelWithEmbeddingOrKernel | None,\n",
        "          data: torch.Tensor,\n",
        "          device: torch.device | None,\n",
        "      ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "          state = self.initialize(model, data, device)\n",
        "\n",
        "          # Debugging: Check sizes\n",
        "          print(f\"Priority queue size: {self.priority_queue.size() if self.priority_queue else 'None'}\")\n",
        "          print(f\"Data size: {data.size(0)}\")\n",
        "\n",
        "          # Reinitialize the priority queue if the size does not match\n",
        "          if self.priority_queue is None or self.priority_queue.size() != data.size(0):\n",
        "              print(\"Reinitializing priority queue to match data size...\")\n",
        "              self.set_initial_priority_queue(\n",
        "                  embeddings=data,\n",
        "                  target_embedding=torch.mean(state.target, dim=0),\n",
        "              )\n",
        "\n",
        "          assert (\n",
        "              self.priority_queue is not None\n",
        "          ), \"Initial priority queue must be set.\"\n",
        "          assert self.priority_queue.size() == data.size(\n",
        "              0\n",
        "          ), f\"Size of the priority queue ({self.priority_queue.size()}) must match the size of the data set ({data.size(0)}).\"\n",
        "\n",
        "          selected_values = []\n",
        "          for _ in range(batch_size):\n",
        "              while True:\n",
        "                  i, _ = self.priority_queue.pop()\n",
        "                  new_value, new_covariance_matrix, state = self.recompute(state, i)\n",
        "\n",
        "                  prev_top_value = self.priority_queue.top_value\n",
        "                  self.priority_queue.push(i, new_value)\n",
        "                  if new_value >= prev_top_value:\n",
        "                      break\n",
        "              selected_values.append(new_value)\n",
        "              state = self.step(state, i, new_covariance_matrix)\n",
        "          return torch.tensor(state.selected_indices), torch.tensor(selected_values)"
      ],
      "metadata": {
        "id": "t2T8YsVb3PQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from activeft.acquisition_functions.lazy_vtl import LazyVTL\n",
        "from activeft.acquisition_functions.bace import BaCEState\n",
        "import os\n",
        "\n",
        "@add_method(SIFTRetriever)\n",
        "def _build_index(self, save_path=\"faiss_index.bin\", load_if_exists=True):\n",
        "    \"\"\"Build FAISS index with SIFT retriever and save it to disk.\"\"\"\n",
        "    if load_if_exists and os.path.exists(save_path):\n",
        "        print(f\"Loading FAISS index from {save_path}...\")\n",
        "        self.faiss_index = faiss.read_index(save_path)\n",
        "        print(f\"Loaded FAISS index with {self.faiss_index.ntotal} embeddings.\")\n",
        "    else:\n",
        "        print(\"Building FAISS index...\")\n",
        "        embeddings = []\n",
        "        valid_chunks = []\n",
        "\n",
        "        for chunk in tqdm(self.chunks):\n",
        "            try:\n",
        "                embedding = self.embedding_model.encode(chunk['text'], convert_to_tensor=True)\n",
        "                embeddings.append(embedding.cpu().numpy())\n",
        "                valid_chunks.append(chunk)\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding chunk: {e}\")\n",
        "                continue\n",
        "\n",
        "        self.chunks = valid_chunks\n",
        "        embeddings = np.array(embeddings).astype('float32')\n",
        "        print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "        faiss.normalize_L2(embeddings)\n",
        "\n",
        "        dimension = embeddings.shape[1]\n",
        "        faiss_index = faiss.IndexFlatIP(dimension)  # Using IP (Inner Product) as per SIFT\n",
        "        faiss_index.add(embeddings)\n",
        "        print(f\"Created FAISS index with {len(embeddings)} embeddings\")\n",
        "\n",
        "        # Save the FAISS index to disk\n",
        "        faiss.write_index(faiss_index, save_path)\n",
        "        print(f\"Saved FAISS index to {save_path}\")\n",
        "\n",
        "        self.faiss_index = faiss_index\n",
        "\n",
        "    # Initialize LazyVTL instead of ITL\n",
        "    lazy_vtl = PatchedLazyVTL(\n",
        "        target=torch.Tensor(),  # Will be set during retrieval\n",
        "        noise_std=self.noise_std,\n",
        "        mini_batch_size=1000,\n",
        "    )\n",
        "\n",
        "    self.retriever = Retriever(\n",
        "        index=self.faiss_index,\n",
        "        llambda=self.lambda_param,\n",
        "        fast=False,\n",
        "        also_query_opposite=self.also_query_opposite,\n",
        "        only_faiss=False,\n",
        "        acquisition_function=lazy_vtl  # Use LazyVTL as the acquisition function\n",
        "    )"
      ],
      "metadata": {
        "id": "hQqXYu9egCME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(SIFTRetriever)\n",
        "def retrieve(self, query: str) -> List[Dict]:\n",
        "    \"\"\"Retrieve chunks using SIFT with optional reranking\"\"\"\n",
        "    query_embedding = self.embedding_model.encode(query).astype('float32')\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    D, I, V, retrieval_time = self.retriever.search(\n",
        "        query=query_embedding,\n",
        "        N=self.k if not self.rerank else self.k * 2,  # Retrieve more if reranking\n",
        "        K=None,\n",
        "        mean_pooling=False,\n",
        "        threads=self.threads\n",
        "    )\n",
        "\n",
        "    # Handle both single result and array results\n",
        "    if isinstance(I, np.int64):\n",
        "        indices = [I]\n",
        "        scores = [D]\n",
        "    else:\n",
        "        indices = I[0] if I.ndim > 1 else I\n",
        "        scores = D[0] if D.ndim > 1 else D\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(indices, scores):\n",
        "        chunk = self.chunks[idx]\n",
        "        results.append({\n",
        "            'text': chunk['text'],\n",
        "            'score': float(score),\n",
        "            'metadata': chunk['metadata']\n",
        "        })\n",
        "\n",
        "    if self.rerank:\n",
        "        results = self.reranker.rerank(query, results, self.top_k_rerank)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "aHlJ8MqqzUmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    rerank = True\n",
        "    top_k = 5\n",
        "    chunk_size = 256\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = SIFTRetriever(\n",
        "        k=20 if rerank else top_k,\n",
        "        chunk_size=chunk_size,\n",
        "        rerank=rerank,\n",
        "        top_k_rerank=top_k\n",
        "    )\n",
        "\n",
        "    # Load corpus\n",
        "    corpus_path = '/content/corpus.json'\n",
        "    retriever.load_corpus(corpus_path)\n",
        "\n",
        "    # Load dataset from Hugging Face\n",
        "    print(\"Loading dataset from Hugging Face...\")\n",
        "    dataset = load_dataset(\"yobro4619/multihop_rag_balanced_sample\")\n",
        "    queries = dataset['train']\n",
        "\n",
        "    results = []\n",
        "    print(\"Performing retrieval...\")\n",
        "    for query_item in tqdm(queries):\n",
        "        retrieved_docs = retriever.retrieve(query_item['query'])\n",
        "\n",
        "        result = {\n",
        "            'query': query_item['query'],\n",
        "            'answer': query_item['answer'],\n",
        "            'question_type': query_item['question_type'],\n",
        "            'retrieval_list': retrieved_docs,\n",
        "            'gold_list': query_item.get('evidence_list', [])\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    output_file = '/content/sift_.LazyVTL.json'\n",
        "    print(f\"Saving results to {output_file}\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "rbhdf8t_zVAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyqBrLufzZfX",
        "outputId": "ee12ad75-9208-4b04-d113-2b8d2fe27d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpus...\n",
            "Processing documents and creating chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 609/609 [00:03<00:00, 162.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6495/6495 [06:27<00:00, 16.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings shape: (6495, 1024)\n",
            "Created FAISS index with 6495 embeddings\n",
            "Saved FAISS index to faiss_index.bin\n",
            "Loading dataset from Hugging Face...\n",
            "Performing retrieval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/24 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 1/24 [00:29<11:08, 29.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 2/24 [00:42<07:19, 19.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 3/24 [00:56<06:02, 17.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 4/24 [01:10<05:17, 15.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 5/24 [01:42<06:50, 21.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 6/24 [02:04<06:32, 21.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 7/24 [02:18<05:27, 19.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 8/24 [02:32<04:39, 17.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 9/24 [02:45<04:04, 16.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 10/24 [02:59<03:36, 15.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 11/24 [03:17<03:31, 16.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 12/24 [03:35<03:19, 16.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 13/24 [03:50<02:59, 16.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 14/24 [04:10<02:55, 17.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▎   | 15/24 [04:26<02:31, 16.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 16/24 [04:39<02:06, 15.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 17/24 [04:59<01:59, 17.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 18/24 [05:12<01:34, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 19/24 [05:25<01:15, 15.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 20/24 [05:37<00:56, 14.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 21/24 [05:50<00:41, 13.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 22/24 [06:05<00:28, 14.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 23/24 [06:19<00:13, 13.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priority queue size: 280\n",
            "Data size: 1000\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 1000\n",
            "Priority queue size: 1000\n",
            "Data size: 495\n",
            "Reinitializing priority queue to match data size...\n",
            "Priority queue size: 495\n",
            "Data size: 280\n",
            "Reinitializing priority queue to match data size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [06:32<00:00, 16.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/sift_.LazyVTL.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQ76yVTq0eco"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}